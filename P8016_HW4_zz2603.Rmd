---
title: "Homework 4"
author: "Ziyi Zhao"
date: "4/26/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lasso2)
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(gbm)

```

# Part 1

## part a)

Fit a regression tree with lpsa as response variable and the other predictors as predictors. Use CV to determine the optimal tree size. Which tree size corresponds to the lowest cv error? Is this the same as the tree size obtained using 1 SE rules?

```{r echo=FALSE}
data(Prostate)

dat <- janitor::clean_names(Prostate) %>% drop_na()

ctrl1 <- trainControl(method = "cv")

set.seed(1)
rpart2.fit <- train(lpsa~.,data = dat,
                   method = "rpart2",
                   tuneGrid = data.frame(maxdepth = 2:10),
                   trControl = ctrl1)

ggplot(rpart2.fit,highlight = TRUE)

rpart2.fit$bestTune



```

Using the cross validation, we can find out that the tree size = 6 correspond to the lowest CV error.

```{r 1SE,echo=FALSE}
set.seed(1)
tree1 <- rpart(formula = lpsa~.,data = dat) 

cpTable <- printcp(tree1)
plotcp(tree1)

minErr <- which.min(cpTable[,4])
tree2 <- prune(tree1,cp=cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])
plotcp(tree2)

```

Using the 1 SE rule, we can see the tree size = 3 has the lowest x-error.

The tree size obtained by using cross validation is different from the tree size obtained by using 1 SE rule.

## part b)

Create a plot of final tree you choose. Pick one of the terminal nodes, and interpret the information displayed.

I'd choose the tree created by 1 SE rules due to relatively smaller cross validation error.

```{r plot, echo=FALSE}
rpart.plot(tree2)

```

I choose the terminal node lcavol < 2.5. If log(cancer volumn) is smaller than 2.5, there is 78% chance for log(prostate specific antigen) to be 2.1. If the log(cancer volumn) is greater than 2.5, there is 22% chance for log(prostate specific antigen) to be 3.8.

## part c)

Perform bagging and report the variable importance.

```{r echo=FALSE}
set.seed(1)
bag.fit <- train(lpsa~., dat,
                 method = "rf",
                 trControl = ctrl1)

set.seed(1)
bag.final.per <- ranger(lpsa~.,dat,
                        mtry=8,splitrule="variance",
                        min.node.size = 7,
                        importance="permutation",
                        scale.permutation.importance=TRUE)

barplot(sort(ranger::importance(bag.final.per),decreasing = FALSE),
        las=2,horiz = TRUE,cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

```

The log(cancer volumn) has the greatest relative importance (apporximate to 35). The age has the least imporance in the model.

## part d)

Perform random forest and the variable importance.

```{r echo=FALSE}
rf.grid <- expand.grid(mtry=1:8,
                      splitrule = "variance",
                      min.node.size = 1:8)

set.seed(1)
rf.fit <- train(lpsa~., dat,
                 method = "ranger",
                 tuneGrid = rf.grid,
                 trControl = ctrl1)

ggplot(bag.fit,highlight = TRUE)

set.seed(1)
rf.final.per <- ranger(lpsa~.,dat,
                       mtry = 3,splitrule = "variance",
                       min.node.size = 7,
                       importance = "impurity")

barplot(sort(ranger::importance(rf.final.per),decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))


```

The log(cancer volumn) has the greatest variable importance (>40) and gleason score has the smallest. 

## part E

Perform boosting and report variable importance

```{r echo=FALSE}
gbm.grid <- expand.grid(n.trees = c(1000,2000),
                        interaction.depth = 1:8,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)

set.seed(1)
gbm.fit <- train(lpsa~.,dat,method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl1,
                 verbose = FALSE)

ggplot(gbm.fit,highlight = TRUE)

summary(gbm.fit$finalModel,las = 2,cBars = 19,cex.names = 0.6)

```

The log(cancer volumn) has the greatest relative influence among all predictors (51.53). The gleason score has the smallest (1.70).

## Part F

Which model will you select to predict PSA level? Explain.

```{r echo=FALSE}
resamp <- resamples(list(bag = bag.fit, rf = rf.fit, gbm = gbm.fit))
summary(resamp)

```

The random forest has the smallest median and mean of RMSE; however, the boosting has larger mean and median of Rsquared. I prefer choosing random forest.

# Part 2

## Fit a classification tree to the training set, with Purchase as the response and the other variables as predictors. Use cross validation to determine the tree size and create a plot of the final tree. Predicted the response on the test data. What's the classification error rate?

```{r}




```





